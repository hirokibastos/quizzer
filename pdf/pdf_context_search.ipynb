{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install all packages and libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain\n",
    "# %pip install huggingface_hub\n",
    "# %pip install sentence_transformers\n",
    "# %pip install faiss-cpu\n",
    "# %pip install unstructured\n",
    "# %pip install chromadb\n",
    "# %pip install Cython\n",
    "# %pip install tiktoken\n",
    "# %pip install unstructured[local-inference]\n",
    "# %pip install pypdf\n",
    "# %pip install ipywidgets\n",
    "# %pip install unstructured"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hugging Face Acess Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not import azure.core python package.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader  #for textfiles\n",
    "from langchain.text_splitter import CharacterTextSplitter #text splitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings #for using HugginFace models\n",
    "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
    "from langchain.vectorstores import FAISS  #facebook vectorizationfrom langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.document_loaders import UnstructuredPDFLoader  #load pdf\n",
    "from langchain.indexes import VectorstoreIndexCreator #vectorize db index with chromadb\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import UnstructuredURLLoader  #load urls into docoument-loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare suport function to print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "    return wrapped_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk to PDF using a combination of FAISS and VectorstoreIndexCreator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PDFs\n",
    "\n",
    "We load pdfs in two different ways, because each one wields a better result when building each of the two vector stores we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all pdfs using PyPDFDirectoryLoader -> This will be used by FAISS\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "loader = PyPDFDirectoryLoader(\"apostila_biologia/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all pdfs using UnstructuredPDFLoader -> This will be used by VectorstoreIndexCreator\n",
    "import os\n",
    "pdf_folder_path = 'apostila_biologia/'\n",
    "os.listdir(pdf_folder_path)\n",
    "loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vectore Stores\n",
    "\n",
    "Doing a similarity search on the pdfs using FAISS to get the document pages for the query first, them using VictorstoreIndexCreator as the retriever in the chain wields best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should take some time (depending on pdf size)\n",
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1138, which is longer than the specified 1000\n",
      "Created a chunk of size 1897, which is longer than the specified 1000\n",
      "Created a chunk of size 1715, which is longer than the specified 1000\n",
      "Created a chunk of size 3771, which is longer than the specified 1000\n",
      "Created a chunk of size 1297, which is longer than the specified 1000\n",
      "Created a chunk of size 2006, which is longer than the specified 1000\n",
      "Created a chunk of size 2777, which is longer than the specified 1000\n",
      "Created a chunk of size 1752, which is longer than the specified 1000\n",
      "Created a chunk of size 3268, which is longer than the specified 1000\n",
      "Created a chunk of size 1054, which is longer than the specified 1000\n",
      "Created a chunk of size 1011, which is longer than the specified 1000\n",
      "Created a chunk of size 2214, which is longer than the specified 1000\n",
      "Created a chunk of size 1146, which is longer than the specified 1000\n",
      "Created a chunk of size 1127, which is longer than the specified 1000\n",
      "Created a chunk of size 1234, which is longer than the specified 1000\n",
      "Created a chunk of size 1302, which is longer than the specified 1000\n",
      "Created a chunk of size 1127, which is longer than the specified 1000\n",
      "Created a chunk of size 1095, which is longer than the specified 1000\n",
      "Created a chunk of size 1601, which is longer than the specified 1000\n",
      "Created a chunk of size 1325, which is longer than the specified 1000\n",
      "Created a chunk of size 1011, which is longer than the specified 1000\n",
      "Created a chunk of size 1023, which is longer than the specified 1000\n",
      "Created a chunk of size 1177, which is longer than the specified 1000\n",
      "Created a chunk of size 1112, which is longer than the specified 1000\n",
      "Created a chunk of size 1044, which is longer than the specified 1000\n",
      "Created a chunk of size 1415, which is longer than the specified 1000\n",
      "Created a chunk of size 1795, which is longer than the specified 1000\n",
      "Created a chunk of size 1084, which is longer than the specified 1000\n",
      "Created a chunk of size 1142, which is longer than the specified 1000\n",
      "Created a chunk of size 1367, which is longer than the specified 1000\n",
      "Created a chunk of size 1091, which is longer than the specified 1000\n",
      "Created a chunk of size 1381, which is longer than the specified 1000\n",
      "Created a chunk of size 1413, which is longer than the specified 1000\n",
      "Created a chunk of size 1429, which is longer than the specified 1000\n",
      "Created a chunk of size 1293, which is longer than the specified 1000\n",
      "Created a chunk of size 1257, which is longer than the specified 1000\n",
      "Created a chunk of size 1314, which is longer than the specified 1000\n",
      "Created a chunk of size 1249, which is longer than the specified 1000\n",
      "Created a chunk of size 1056, which is longer than the specified 1000\n",
      "Created a chunk of size 1415, which is longer than the specified 1000\n",
      "Created a chunk of size 1425, which is longer than the specified 1000\n",
      "Created a chunk of size 1437, which is longer than the specified 1000\n",
      "Created a chunk of size 1392, which is longer than the specified 1000\n",
      "Created a chunk of size 1015, which is longer than the specified 1000\n",
      "Created a chunk of size 1840, which is longer than the specified 1000\n",
      "Created a chunk of size 1005, which is longer than the specified 1000\n",
      "Created a chunk of size 1168, which is longer than the specified 1000\n",
      "Created a chunk of size 1508, which is longer than the specified 1000\n",
      "Created a chunk of size 1215, which is longer than the specified 1000\n",
      "Created a chunk of size 1047, which is longer than the specified 1000\n",
      "Created a chunk of size 1539, which is longer than the specified 1000\n",
      "Created a chunk of size 1015, which is longer than the specified 1000\n",
      "Created a chunk of size 1171, which is longer than the specified 1000\n",
      "Created a chunk of size 1023, which is longer than the specified 1000\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# This should take some time (depending on pdf size)\n",
    "# Chunk size warnings here can be ignored\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model repo name\n",
    "# Must be text2text-generation or text-generation model\n",
    "# Change this to use other LLMs\n",
    "# Find them here: https://huggingface.co/models?pipeline_tag=text2text-generation&sort=downloads\n",
    "# Model MUST have it's Hosted Inference API active\n",
    "# If a model is too large, it's more likely the API is going to timeout\n",
    "\n",
    "model_repo_name = \"unicamp-dl/ptt5-base-portuguese-vocab\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "#Load llm with selected one\n",
    "llm=HuggingFaceHub(repo_id=model_repo_name, model_kwargs={\"temperature\":0, \"max_length\":512})\n",
    "#Prepare the pipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=index.vectorstore.as_retriever(),\n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should return pages containing the answer to our query\n",
    "query = \"Qual a diferença de floema e xilema?\"\n",
    "pages = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quando faltam elementos fl orais (também denominados verticilos), a fl or é chamada de incompleta. Quando um\n",
      "dos verticilos férteis está ausente, a fl or é denominada díclina: ou possui apenas gineceu (fl or pistilada),\n",
      "ou possui apenas androceu (fl or estaminada). A fl or que possui gineceu e androceu é denominada monóclina.\n",
      "Flor monóclina Flores díclinas Pistilada e estaminada Flor pistilada Flor estaminada Possui estruturas\n",
      "femininas (car- pelos) e masculinas (estames) Possui somente estruturas femininas (carpelos) Possui somente\n",
      "estruturas masculinas (estames) Fonte: elaborada pela autora.\n",
      "Fonte:https://commons.wikimedia.org/wiki/File:Flower_morphology_sex_staminate.png, https://commons.\n",
      "wikimedia.org/wiki/File:Flower_morphology_attachment_pedicellate., https://commons.wikimedia.org/wiki/File:-\n",
      "Flower_morphology_sex_pistillate.pngpng?uselang=pt-br Figura 69: Flor da família Magnoliaceae. Ao centro,\n",
      "vários carpelos. Fonte: http://www.sxc.hu/photo/815609 Figura70: Flores da família Liliacea. Ao centro da fl\n",
      "or, os carpelos se alargam na ponta, onde (em cor avermelhada) se localiza o estigma (estrutura feminina).\n",
      "Fonte:http://www.sxc.hu/photo/1246314;https://commons.wikimedia.org/wiki/Lilium#/media/File:Lilium_%-\n",
      "27Gran_Paradiso%27.jpg Algumas fl ores não se apresentam únicas nos ramos, mas em grupos, o que é denominado\n",
      "“infl orescência”. Ou seja, a infl\n"
     ]
    }
   ],
   "source": [
    "# Pass pages to LLM to build an answer\n",
    "# First query should take some time\n",
    "answer = chain.run(input_documents=pages, question=query)\n",
    "# First sentence (163 characters) is left over from a langchain prompt template so we just throw it out\n",
    "print(wrap_text_preserve_newlines(answer[163:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk to websites using VectorstoreIndexCreator\n",
    "\n",
    "Here we will only use VectorstoreIndexCreator to show another way to use langchain, but this can easily be modified to use FAISS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "urls = [\n",
    "    \"https://pt.wikipedia.org/wiki/Mitose\",\n",
    "    \"https://pt.wikipedia.org/wiki/Meiose\",\n",
    "    \"https://pt.wikipedia.org/wiki/Divis%C3%A3o_celular\",\n",
    "    \"https://pt.wikipedia.org/wiki/Sistema_imunit%C3%A1rio\",\n",
    "    \"https://pt.wikipedia.org/wiki/Mam%C3%ADferos\",\n",
    "    \"https://pt.wikipedia.org/wiki/Floema\",\n",
    "    \"https://pt.wikipedia.org/wiki/Xilema\",\n",
    "    \"https://pt.wikipedia.org/wiki/Angiosperma\",\n",
    "]\n",
    "loader_url = [UnstructuredURLLoader(urls=urls)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Store using VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1096, which is longer than the specified 1000\n",
      "Created a chunk of size 1717, which is longer than the specified 1000\n",
      "Created a chunk of size 1860, which is longer than the specified 1000\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "index_url = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loader_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model repo name\n",
    "# Must be text2text-generation or text-generation model\n",
    "# Change this to use other LLMs\n",
    "# Find them here: https://huggingface.co/models?pipeline_tag=text2text-generation&sort=downloads\n",
    "# Model MUST have it's Hosted Inference API active\n",
    "# If a model is too large, it's more likely the API is going to timeout\n",
    "\n",
    "model_repo_name_url = \"unicamp-dl/ptt5-base-portuguese-vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "llm_url=HuggingFaceHub(repo_id=model_repo_name_url, model_kwargs={\"temperature\":0, \"max_length\":512})\n",
    "\n",
    "# Create chain\n",
    "from langchain.chains import RetrievalQA\n",
    "chain_url = RetrievalQA.from_chain_type(llm=llm_url, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=index_url.vectorstore.as_retriever(), \n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os produtos transportados pelo floema são substâncias inorgânicas e orgânicas, como água, lipídios e\n",
      "carboidratos, são transportados desde os órgãos da planta com capacidade fotossintética (ou produtores), como\n",
      "folhas maduras, até outros que funcionam como consumidores dessas substâncias, para a formação de novos órgãos\n",
      "ou para reserva, nomeadamente, os meristemas, as células do interior do caule, da raiz, das flores, dos frutos\n",
      "e dos órgãos de reserva - que podem estar dispersos dentro do caule e da raiz, mas que podem estar\n",
      "especializados, como os tubérculos e rizomas. Ocorrência[editar | editar código-fonte] O floema está presente\n",
      "praticamente em toda fase da vida da planta, tanto estrutura primária, na qual a planta ainda está em sua\n",
      "forma jovem, quanto em estrutura secundária na qual os órgãos adquirem uma certa resistência. Ocorre em todas\n",
      "as partes da planta: caule, raiz, folha, partes florais etc. Normalmente, durante o crescimento primário (em\n",
      "altura), o floema e o xilema se alternam - isto acontece devido à desorganização dos órgãos das plantas. Já\n",
      "durante o crescimento secundário (espessura), o floema fica mais externamente e o xilema mais internamente. Em\n",
      "alguns casos de famílias de dicotiledôneas, como Apocynaceae, Asteraceae, Curcubitaceae, apresentam um floema\n",
      "adicional interno ao xilema, chamado de floema incluso, interno ou intraxilemático, devido ao crescimento em\n",
      "excesso de algum órgão em espessura. Em órgãos como folhas, a posição do floema é dorsal. Em plantas com\n",
      "crescimento secundário, o floema é parte do córtex ou \"casca primária\" e o termo floema deriva da palavra\n",
      "grega para \"casca\". Células do floema[editar | editar código-fonte] O floema é formado por células alongadas,\n",
      "cilíndricas, provenientes de células do procâmbio quando estão em crescimento primário, ou pelo câmbio\n",
      "vascular que forma o floema secundário\n"
     ]
    }
   ],
   "source": [
    "query_url = \"Qual a função do floema?\"\n",
    "answer_url = chain_url.run(query_url)\n",
    "# First sentence (163 characters) is left over from a langchain prompt template so we just throw it out\n",
    "print(wrap_text_preserve_newlines(answer_url[163:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencia:\n",
    "\n",
    "https://www.python-engineer.com/posts/langchain-crash-course/\n",
    "\n",
    "https://python.langchain.com/en/latest/modules/models/llms/integrations/huggingface_hub.html\n",
    "\n",
    "https://python.langchain.com/en/latest/_modules/langchain/embeddings/huggingface.html\n",
    "\n",
    "https://medium.com/the-techlife/using-huggingface-openai-and-cohere-models-with-langchain-db57af14ac5b\n",
    "\n",
    "https://python.langchain.com/en/latest/reference/modules/embeddings.html\n",
    "\n",
    "https://artificialcorner.com/answering-question-about-your-documents-using-langchain-and-not-openai-2f75b8d639ae\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFG",
   "language": "python",
   "name": "pfg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
